{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9544731,"sourceType":"datasetVersion","datasetId":5814832}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"_uuid":"6e3ee215-357c-4302-9851-bd893a781167","_cell_guid":"f49b309c-6a9c-4ea3-8c75-33e41998115b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T12:12:46.049595Z","iopub.execute_input":"2024-12-02T12:12:46.049969Z","iopub.status.idle":"2024-12-02T12:12:51.784334Z","shell.execute_reply.started":"2024-12-02T12:12:46.049916Z","shell.execute_reply":"2024-12-02T12:12:51.783300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RealTimeConfig:\n    \"\"\"Enhanced configuration for performance\"\"\"\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    IMAGE_SIZE = 256  # Increased from 224\n    BATCH_SIZE = 16   # Reduced batch size for more detailed learning\n    LEARNING_RATE = 1e-3\n    NUM_EPOCHS = 150  # Increased epochs\n    PATIENCE = 20\n    RANDOM_SEED = 42\n    INFERENCE_THRESHOLD = 0.5","metadata":{"_uuid":"b947cc20-e3d4-470f-9f79-2998110dcbba","_cell_guid":"60f4cc5d-aaef-43fd-9e5b-1d3fd639fccb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T12:12:51.786081Z","iopub.execute_input":"2024-12-02T12:12:51.787236Z","iopub.status.idle":"2024-12-02T12:12:51.820720Z","shell.execute_reply.started":"2024-12-02T12:12:51.787193Z","shell.execute_reply":"2024-12-02T12:12:51.819738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TrainingLogger:\n    \"\"\"Advanced training and validation logging\"\"\"\n    def __init__(self, save_dir='/kaggle/working'):\n        self.save_dir = save_dir\n        os.makedirs(save_dir, exist_ok=True)\n        \n        # Metrics tracking\n        self.train_losses = []\n        self.val_losses = []\n        self.train_dice = []\n        self.val_dice = []\n        self.train_iou = []\n        self.val_iou = []\n        self.train_precision = []\n        self.val_precision = []\n        self.train_recall = []\n        self.val_recall = []\n        self.train_f1 = []\n        self.val_f1 = []\n\n    def log_epoch(self, \n                  train_loss, val_loss, \n                  train_metrics, val_metrics):\n        \"\"\"Log metrics for each epoch\"\"\"\n        # Losses\n        self.train_losses.append(train_loss)\n        self.val_losses.append(val_loss)\n        \n        # Dice Coefficient\n        self.train_dice.append(train_metrics['dice'])\n        self.val_dice.append(val_metrics['dice'])\n        \n        # IoU\n        self.train_iou.append(train_metrics['iou'])\n        self.val_iou.append(val_metrics['iou'])\n        \n        # Precision\n        self.train_precision.append(train_metrics['precision'])\n        self.val_precision.append(val_metrics['precision'])\n        \n        # Recall\n        self.train_recall.append(train_metrics['recall'])\n        self.val_recall.append(val_metrics['recall'])\n        \n        # F1 Score\n        self.train_f1.append(train_metrics['f1'])\n        self.val_f1.append(val_metrics['f1'])\n\n    def plot_metrics(self):\n        \"\"\"Visualize training metrics\"\"\"\n        metrics_to_plot = [\n            ('Loss', self.train_losses, self.val_losses),\n            ('Dice Coefficient', self.train_dice, self.val_dice),\n            ('IoU Score', self.train_iou, self.val_iou),\n            ('Precision', self.train_precision, self.val_precision),\n            ('Recall', self.train_recall, self.val_recall),\n            ('F1 Score', self.train_f1, self.val_f1)\n        ]\n\n        plt.figure(figsize=(15, 10))\n        for i, (title, train_data, val_data) in enumerate(metrics_to_plot, 1):\n            plt.subplot(2, 3, i)\n            plt.plot(train_data, label=f'Train {title}')\n            plt.plot(val_data, label=f'Validation {title}')\n            plt.title(f'{title} Over Epochs')\n            plt.xlabel('Epoch')\n            plt.ylabel(title)\n            plt.legend()\n\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.save_dir, 'training_metrics.png'))\n        plt.close()\n\n    def save_metrics(self):\n        \"\"\"Save metrics to CSV for further analysis\"\"\"\n        import pandas as pd\n        \n        metrics_df = pd.DataFrame({\n            'Train Loss': self.train_losses,\n            'Val Loss': self.val_losses,\n            'Train Dice': self.train_dice,\n            'Val Dice': self.val_dice,\n            'Train IoU': self.train_iou,\n            'Val IoU': self.val_iou,\n            'Train Precision': self.train_precision,\n            'Val Precision': self.val_precision,\n            'Train Recall': self.train_recall,\n            'Val Recall': self.val_recall,\n            'Train F1': self.train_f1,\n            'Val F1': self.val_f1\n        })\n        \n        metrics_df.to_csv(os.path.join(self.save_dir, 'training_metrics.csv'), index=False)","metadata":{"_uuid":"611b14dc-1dc5-4f25-998d-a675b71b3a8b","_cell_guid":"a69e554b-09e4-42cf-a744-f94f2f61b634","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T12:12:51.821800Z","iopub.execute_input":"2024-12-02T12:12:51.822059Z","iopub.status.idle":"2024-12-02T12:12:51.836674Z","shell.execute_reply.started":"2024-12-02T12:12:51.822035Z","shell.execute_reply":"2024-12-02T12:12:51.835867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SegmentationMetrics:\n    \"\"\"Comprehensive metrics for segmentation performance\"\"\"\n    @staticmethod\n    def dice_coefficient(pred, target, smooth=1e-7):\n        \"\"\"Calculate Dice Coefficient\"\"\"\n        pred = pred.view(-1)\n        target = target.view(-1)\n        intersection = (pred * target).sum()\n        return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n\n    @staticmethod\n    def iou_score(pred, target, smooth=1e-7):\n        \"\"\"Calculate Intersection over Union (IoU)\"\"\"\n        pred = pred.view(-1)\n        target = target.view(-1)\n        intersection = (pred * target).sum()\n        total = (pred + target).sum()\n        union = total - intersection\n        return (intersection + smooth) / (union + smooth)\n\n    @staticmethod\n    def precision(pred, target, smooth=1e-7):\n        \"\"\"Calculate Precision\"\"\"\n        pred = pred.view(-1)\n        target = target.view(-1)\n        true_positives = (pred * target).sum()\n        return (true_positives + smooth) / (pred.sum() + smooth)\n\n    @staticmethod\n    def recall(pred, target, smooth=1e-7):\n        \"\"\"Calculate Recall\"\"\"\n        pred = pred.view(-1)\n        target = target.view(-1)\n        true_positives = (pred * target).sum()\n        return (true_positives + smooth) / (target.sum() + smooth)\n\n    @staticmethod\n    def f1_score(precision, recall):\n        \"\"\"Calculate F1 Score\"\"\"\n        return 2 * (precision * recall) / (precision + recall + 1e-7)","metadata":{"_uuid":"6e9e08c6-14aa-4a9f-8365-405c6d97ad83","_cell_guid":"ea0e1f4f-e055-467d-b7e4-fb4a66ea09a5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T12:12:51.838802Z","iopub.execute_input":"2024-12-02T12:12:51.839174Z","iopub.status.idle":"2024-12-02T12:12:51.853545Z","shell.execute_reply.started":"2024-12-02T12:12:51.839134Z","shell.execute_reply":"2024-12-02T12:12:51.852718Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image Preprocessing Utilities\nclass ImagePreprocessor:\n    \"\"\"Advanced image preprocessing techniques\"\"\"\n    @staticmethod\n    def white_balance(img):\n        \"\"\"Perform white balancing using LAB color space\"\"\"\n        img_LAB = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n        avg_a = np.average(img_LAB[:, :, 1])\n        avg_b = np.average(img_LAB[:, :, 2])\n        img_LAB[:, :, 1] = img_LAB[:, :, 1] - ((avg_a - 128) * (img_LAB[:, :, 0] / 255.0) * 1.2)\n        img_LAB[:, :, 2] = img_LAB[:, :, 2] - ((avg_b - 128) * (img_LAB[:, :, 0] / 255.0) * 1.2)\n        return cv2.cvtColor(img_LAB, cv2.COLOR_LAB2BGR)\n\n    @staticmethod\n    def unsharp_mask(image, radius=2, amount=1):\n        \"\"\"Apply unsharp masking for image sharpening\"\"\"\n        return unsharp_mask(image=image, radius=radius, amount=amount)\n\n    @staticmethod\n    def apply_clahe(image):\n        \"\"\"Apply Contrast Limited Adaptive Histogram Equalization\"\"\"\n        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        l1 = clahe.apply(l)\n        lab_planes = cv2.merge((l1, a, b))\n        bgr = cv2.cvtColor(lab_planes, cv2.COLOR_LAB2BGR)\n        return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n\n    @classmethod\n    def enhance_image(cls, image):\n        \"\"\"Comprehensive image enhancement pipeline\"\"\"\n        white_balanced = cls.white_balance(image)\n        sharpened = cls.unsharp_mask(white_balanced)\n        enhanced = cls.apply_clahe((sharpened * 255).astype(np.uint8))\n        return enhanced, sharpened, white_balanced","metadata":{"_uuid":"7b035776-cd56-47b7-8a96-53ebfef2636e","_cell_guid":"dd3a73f7-1bbf-4192-a143-ba7d1b49b4c5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T12:12:51.854662Z","iopub.execute_input":"2024-12-02T12:12:51.854907Z","iopub.status.idle":"2024-12-02T12:12:51.871604Z","shell.execute_reply.started":"2024-12-02T12:12:51.854883Z","shell.execute_reply":"2024-12-02T12:12:51.870681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Custom Dataset\nclass KidneyDataset(Dataset):\n    def __init__(self, filenames, image_dir, masks_dir, transform=None):\n        self.filenames = filenames\n        self.image_dir = image_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.filenames[idx])\n        mask_path = os.path.join(self.masks_dir, self.filenames[idx])\n\n        # Read images\n        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n\n        # Ensure tensor with channel dimension\n        image = image.unsqueeze(0) if len(image.shape) == 2 else image\n        mask = mask.unsqueeze(0) if len(mask.shape) == 2 else mask\n\n        return image, mask","metadata":{"_uuid":"fa16cb92-52e3-40b1-a557-9e826d96fa13","_cell_guid":"f9d6ec7e-a304-427f-8ba6-a50e8656fba5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T12:12:51.872771Z","iopub.execute_input":"2024-12-02T12:12:51.873096Z","iopub.status.idle":"2024-12-02T12:12:51.885226Z","shell.execute_reply.started":"2024-12-02T12:12:51.873053Z","shell.execute_reply":"2024-12-02T12:12:51.884373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SpatialAttention(nn.Module):\n    \"\"\"Improved Spatial Attention Module\"\"\"\n    def __init__(self, in_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // 2, kernel_size=1),\n            nn.BatchNorm2d(in_channels // 2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // 2, 1, kernel_size=7, padding=3),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        # Channel-wise attention\n        attention = self.conv(x)\n        # Scale the input with the attention map\n        return x * attention","metadata":{"_uuid":"bdb30d1d-1cb6-4c1b-ab50-e166026520bc","_cell_guid":"fa6ab408-098b-4b5c-80de-bc00a7d87a75","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-02T12:12:52.969899Z","iopub.execute_input":"2024-12-02T12:12:52.970887Z","iopub.status.idle":"2024-12-02T12:12:52.976774Z","shell.execute_reply.started":"2024-12-02T12:12:52.970852Z","shell.execute_reply":"2024-12-02T12:12:52.975545Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EnhancedUNet(nn.Module):\n    \"\"\"Advanced UNet with attention and residual connections\"\"\"\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n        \n        def conv_block(in_ch, out_ch, dropout_rate=0.2):\n            return nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out_ch),\n                nn.ReLU(inplace=True),\n                nn.Dropout2d(dropout_rate),\n                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out_ch),\n                nn.ReLU(inplace=True)\n            )\n        \n        # Encoder with increased complexity\n        self.enc1 = conv_block(in_channels, 64)\n        self.pool1 = nn.MaxPool2d(2)\n        \n        self.enc2 = conv_block(64, 128)\n        self.pool2 = nn.MaxPool2d(2)\n        \n        self.enc3 = conv_block(128, 256)\n        self.pool3 = nn.MaxPool2d(2)\n        \n        self.enc4 = conv_block(256, 512)\n        self.pool4 = nn.MaxPool2d(2)\n        \n        # Bridge with spatial attention\n        self.bridge = nn.Sequential(\n            conv_block(512, 1024),\n            SpatialAttention(1024)\n        )\n        \n        # Decoder with skip connections and residual blocks\n        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.dec4 = conv_block(1024, 512)\n        \n        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec3 = conv_block(512, 256)\n        \n        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = conv_block(256, 128)\n        \n        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = conv_block(128, 64)\n        \n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n    \n    def forward(self, x):\n        # Ensure input has the right number of channels\n        if x.size(1) != 1:\n            x = x[:, 0:1, :, :]  # Take only the first channel if multiple are present\n        \n        # Encoder\n        enc1 = self.enc1(x)\n        pool1 = self.pool1(enc1)\n        \n        enc2 = self.enc2(pool1)\n        pool2 = self.pool2(enc2)\n        \n        enc3 = self.enc3(pool2)\n        pool3 = self.pool3(enc3)\n        \n        enc4 = self.enc4(pool3)\n        pool4 = self.pool4(enc4)\n        \n        # Bridge\n        bridge = self.bridge(pool4)\n        \n        # Decoder with skip connections\n        upconv4 = self.upconv4(bridge)\n        concat4 = torch.cat([upconv4, enc4], dim=1)\n        dec4 = self.dec4(concat4)\n        \n        upconv3 = self.upconv3(dec4)\n        concat3 = torch.cat([upconv3, enc3], dim=1)\n        dec3 = self.dec3(concat3)\n        \n        upconv2 = self.upconv2(dec3)\n        concat2 = torch.cat([upconv2, enc2], dim=1)\n        dec2 = self.dec2(concat2)\n        \n        upconv1 = self.upconv1(dec2)\n        concat1 = torch.cat([upconv1, enc1], dim=1)\n        dec1 = self.dec1(concat1)\n        \n        return torch.sigmoid(self.final_conv(dec1))","metadata":{"_uuid":"d5473f0f-057a-44db-abdf-29665ae2ea91","_cell_guid":"260f2acf-18e5-449f-9113-0914c7b2c39e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T12:12:53.270973Z","iopub.execute_input":"2024-12-02T12:12:53.271320Z","iopub.status.idle":"2024-12-02T12:12:53.284800Z","shell.execute_reply.started":"2024-12-02T12:12:53.271292Z","shell.execute_reply":"2024-12-02T12:12:53.283706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def focal_loss(pred, target, alpha=0.25, gamma=2.0):\n    \"\"\"Improved Focal Loss for imbalanced segmentation\"\"\"\n    BCE_loss = F.binary_cross_entropy(pred, target, reduction='none')\n    pt = torch.exp(-BCE_loss)\n    F_loss = alpha * (1-pt)**gamma * BCE_loss\n    return torch.mean(F_loss)","metadata":{"_uuid":"f5e540b1-4fa6-40fd-b89e-f002057b5d26","_cell_guid":"85d45d37-3163-4fbd-b845-1c83d99fd78c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T12:12:53.589889Z","iopub.execute_input":"2024-12-02T12:12:53.590956Z","iopub.status.idle":"2024-12-02T12:12:53.596373Z","shell.execute_reply.started":"2024-12-02T12:12:53.590893Z","shell.execute_reply":"2024-12-02T12:12:53.595436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, config=RealTimeConfig()):\n    device = config.DEVICE\n    model = model.to(device)\n    \n    optimizer = torch.optim.AdamW(\n        model.parameters(), \n        lr=config.LEARNING_RATE, \n        weight_decay=1e-4  # Added weight decay for regularization\n    )\n    \n    # Learning rate scheduler\n    scheduler = ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.5, \n        patience=5, \n        verbose=True\n    )\n    \n    logger = TrainingLogger()\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.NUM_EPOCHS):\n        model.train()\n        train_losses = []\n        train_metrics = {\n            'dice': [], 'iou': [], \n            'precision': [], 'recall': [], 'f1': []\n        }\n        \n        for images, masks in train_loader:\n            \n            # print(\"Image tensor shape:\", images.shape)\n            # print(\"Mask tensor shape:\", masks.shape)\n  \n            images, masks = images.to(device), masks.to(device)\n            \n            optimizer.zero_grad()\n            predictions = model(images)\n            \n            # Combine Focal Loss and Dice Loss\n            focal = focal_loss(predictions, masks)\n            dice = 1 - SegmentationMetrics.dice_coefficient(predictions, masks)\n            loss = 0.6 * focal + 0.4 * dice\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n            optimizer.step()\n            \n            # Compute metrics\n            pred_binary = (predictions > 0.5).float()\n            dice_metric = SegmentationMetrics.dice_coefficient(pred_binary, masks)\n            iou = SegmentationMetrics.iou_score(pred_binary, masks)\n            precision = SegmentationMetrics.precision(pred_binary, masks)\n            recall = SegmentationMetrics.recall(pred_binary, masks)\n            f1 = SegmentationMetrics.f1_score(precision, recall)\n            \n            train_losses.append(loss.item())\n            train_metrics['dice'].append(dice_metric.item())\n            train_metrics['iou'].append(iou.item())\n            train_metrics['precision'].append(precision.item())\n            train_metrics['recall'].append(recall.item())\n            train_metrics['f1'].append(f1.item())\n        \n        # Validation phase (similar to training phase)\n        model.eval()\n        val_losses = []\n        val_metrics = {\n            'dice': [], 'iou': [], \n            'precision': [], 'recall': [], 'f1': []\n        }\n        \n        with torch.no_grad():\n            for images, masks in val_loader:\n                images, masks = images.to(device), masks.to(device)\n                predictions = model(images)\n                \n                focal = focal_loss(predictions, masks)\n                dice = 1 - SegmentationMetrics.dice_coefficient(predictions, masks)\n                loss = 0.6 * focal + 0.4 * dice\n                \n                pred_binary = (predictions > 0.5).float()\n                dice_metric = SegmentationMetrics.dice_coefficient(pred_binary, masks)\n                iou = SegmentationMetrics.iou_score(pred_binary, masks)\n                precision = SegmentationMetrics.precision(pred_binary, masks)\n                recall = SegmentationMetrics.recall(pred_binary, masks)\n                f1 = SegmentationMetrics.f1_score(precision, recall)\n                \n                val_losses.append(loss.item())\n                val_metrics['dice'].append(dice_metric.item())\n                val_metrics['iou'].append(iou.item())\n                val_metrics['precision'].append(precision.item())\n                val_metrics['recall'].append(recall.item())\n                val_metrics['f1'].append(f1.item())\n        \n        # Compute average metrics (same as before)\n        avg_train_loss = np.mean(train_losses)\n        avg_val_loss = np.mean(val_losses)\n        \n        # Learning rate scheduling\n        scheduler.step(avg_val_loss)\n        \n        # Log and print metrics (similar to previous implementation)\n        train_metrics_avg = {k: np.mean(v) for k, v in train_metrics.items()}\n        val_metrics_avg = {k: np.mean(v) for k, v in val_metrics.items()}\n        \n        logger.log_epoch(avg_train_loss, avg_val_loss, \n                         train_metrics_avg, val_metrics_avg)\n        \n        print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n        print(f\"Train Dice: {train_metrics_avg['dice']:.4f}, Val Dice: {val_metrics_avg['dice']:.4f}\")\n        \n        # Model checkpointing\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), '/kaggle/working/best_kidney_segmentation_model.pth')\n        else:\n            patience_counter += 1\n        \n        # Early stopping\n        if patience_counter >= config.PATIENCE:\n            print(\"Early stopping triggered\")\n            break\n    \n    # Plot and save metrics\n    logger.plot_metrics()\n    logger.save_metrics()\n    \n    return model","metadata":{"_uuid":"66fbdfbb-4e11-4bb5-9750-b4256cd2b6bc","_cell_guid":"5d3c3bd3-792e-40f7-b49d-2efb4bc8a3f2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T12:12:53.884918Z","iopub.execute_input":"2024-12-02T12:12:53.885274Z","iopub.status.idle":"2024-12-02T12:12:53.901069Z","shell.execute_reply.started":"2024-12-02T12:12:53.885246Z","shell.execute_reply":"2024-12-02T12:12:53.900092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    # Data augmentation\n    transform = A.Compose([\n        A.Resize(height=RealTimeConfig.IMAGE_SIZE, width=RealTimeConfig.IMAGE_SIZE),\n        A.OneOf([\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5)\n        ], p=0.7),\n        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5),\n        A.RandomBrightnessContrast(p=0.3),\n        A.ToFloat(max_value=1.0),\n        ToTensorV2()\n    ])\n    \n    # Dataset paths (update these to your specific paths)\n    image_dir = '/kaggle/input/2kdataset/2kdataset/images'\n    masks_dir = '/kaggle/input/2kdataset/2kdataset/masks'\n    \n    # Prepare dataset\n    filenames = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n    train_files, val_files = train_test_split(filenames, test_size=0.2, random_state=RealTimeConfig.RANDOM_SEED)\n    \n    train_dataset = KidneyDataset(train_files, image_dir, masks_dir, transform=transform)\n    val_dataset = KidneyDataset(val_files, image_dir, masks_dir, transform=transform)\n    \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=RealTimeConfig.BATCH_SIZE, \n        shuffle=True, \n        num_workers=4, \n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=RealTimeConfig.BATCH_SIZE, \n        shuffle=False, \n        num_workers=4, \n        pin_memory=True\n    )\n    \n    # Initialize and train model\n    model = EnhancedUNet()\n    trained_model = train_model(model, train_loader, val_loader)\n    \n    print(\"Model training completed successfully!\")","metadata":{"_uuid":"c80c075f-1370-40b8-9af0-da1bdfd443e6","_cell_guid":"049f69f2-820e-4b5a-becc-32a5c58e4755","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T12:12:54.135367Z","iopub.execute_input":"2024-12-02T12:12:54.135699Z","iopub.status.idle":"2024-12-02T12:12:54.143609Z","shell.execute_reply.started":"2024-12-02T12:12:54.135672Z","shell.execute_reply":"2024-12-02T12:12:54.142561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"b9b0f1c7-9f4c-478b-925f-08050f978d7c","_cell_guid":"a1c89667-e638-4d12-b50d-934490839b9b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-02T12:12:54.408966Z","iopub.execute_input":"2024-12-02T12:12:54.409665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"b19a8545-a5fa-4c45-96fd-93e2798797ee","_cell_guid":"aad5a9b6-dddb-45f3-a465-1a9a9875e9ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}