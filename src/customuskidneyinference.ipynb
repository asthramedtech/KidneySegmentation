{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9544731,"sourceType":"datasetVersion","datasetId":5814832},{"sourceId":10109875,"sourceType":"datasetVersion","datasetId":5815283},{"sourceId":167551,"sourceType":"modelInstanceVersion","modelInstanceId":113927,"modelId":137208},{"sourceId":191561,"sourceType":"modelInstanceVersion","modelInstanceId":113927,"modelId":137208}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch\n!pip install torchvision\n!pip install opencv-python\n!pip install albumentations\n!pip install numpy\n!pip install scikit-learn\n!pip install matplotlib\n!pip install pandas","metadata":{"_uuid":"3ac5905f-0da0-4bc7-a148-bd32c676d878","_cell_guid":"e958952d-4fce-45ed-a740-6bfb760f8680","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"karthickjeeva/kidneyseg/pyTorch/default\")\n\nprint(\"Path to model files:\", path)","metadata":{"_uuid":"c45ed552-6bb1-4944-8043-a44a66f7a79c","_cell_guid":"08b5e7e3-ea3d-4098-9476-9d39abe5f8ca","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-31T07:02:30.529126Z","iopub.execute_input":"2025-12-31T07:02:30.529350Z","iopub.status.idle":"2025-12-31T07:02:33.130734Z","shell.execute_reply.started":"2025-12-31T07:02:30.529329Z","shell.execute_reply":"2025-12-31T07:02:33.130060Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kidney Segmentation - Inference Notebook\n# This notebook provides inference capabilities for the trained kidney segmentation model\n\n# ============================================================================\n# SECTION 1: Import Required Libraries\n# ============================================================================\n\nimport os\nimport datetime\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\n\nprint(f\"PyTorch version: {torch.__version__}\")\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ============================================================================\n# SECTION 2: Model Architecture Definitions\n# ============================================================================\n\nclass ResidualConnection(nn.Module):\n    \"\"\"Residual Connection module to handle channel adjustments\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super(ResidualConnection, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else None\n    \n    def forward(self, x, identity):\n        if self.conv is not None:\n            identity = self.conv(identity)\n        return x + identity\n\n\nclass ResidualBlock(nn.Module):\n    \"\"\"Residual Block with convolutional layers and batch normalization\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super(ResidualBlock, self).__init__()\n        \n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.residual = ResidualConnection(in_channels, out_channels)\n        \n    def forward(self, x):\n        identity = x\n        x = self.conv_block(x)\n        return self.residual(x, identity)\n\n\nclass EnhancedUNet(nn.Module):\n    \"\"\"Enhanced UNet with Residual Blocks and Deep Supervision\"\"\"\n    def __init__(self, in_channels=1, out_channels=1):\n        super(EnhancedUNet, self).__init__()\n        \n        # Encoder\n        self.encoder1 = ResidualBlock(in_channels, 64)\n        self.encoder2 = ResidualBlock(64, 128)\n        self.encoder3 = ResidualBlock(128, 256)\n        self.encoder4 = ResidualBlock(256, 512)\n        self.encoder5 = ResidualBlock(512, 1024)\n        \n        # Deep Supervision\n        self.deep_sup4 = nn.Conv2d(512, out_channels, 1)\n        self.deep_sup3 = nn.Conv2d(256, out_channels, 1)\n        self.deep_sup2 = nn.Conv2d(128, out_channels, 1)\n        \n        # Decoder\n        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n        self.decoder4 = ResidualBlock(1024, 512)\n        self.norm4 = nn.LayerNorm([512, 32, 32])\n        \n        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n        self.decoder3 = ResidualBlock(512, 256)\n        self.norm3 = nn.LayerNorm([256, 64, 64])\n        \n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.decoder2 = ResidualBlock(256, 128)\n        self.norm2 = nn.LayerNorm([128, 128, 128])\n        \n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.decoder1 = ResidualBlock(128, 64)\n        self.norm1 = nn.LayerNorm([64, 256, 256])\n        \n        self.final_conv = nn.Conv2d(64, out_channels, 1)\n        self.dropout = nn.Dropout2d(0.3)\n    \n    def forward(self, x):\n        # Encoder\n        enc1 = self.dropout(self.encoder1(x))\n        enc2 = self.dropout(self.encoder2(F.max_pool2d(enc1, 2)))\n        enc3 = self.dropout(self.encoder3(F.max_pool2d(enc2, 2)))\n        enc4 = self.dropout(self.encoder4(F.max_pool2d(enc3, 2)))\n        enc5 = self.dropout(self.encoder5(F.max_pool2d(enc4, 2)))\n        \n        # Decoder\n        dec4 = self.upconv4(enc5)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.norm4(self.decoder4(dec4))\n        deep_out4 = self.deep_sup4(dec4)\n        \n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.norm3(self.decoder3(dec3))\n        deep_out3 = self.deep_sup3(dec3)\n        \n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.norm2(self.decoder2(dec2))\n        deep_out2 = self.deep_sup2(dec2)\n        \n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.norm1(self.decoder1(dec1))\n        \n        final_out = self.final_conv(dec1)\n        \n        return (final_out, deep_out4, deep_out3, deep_out2) if self.training else final_out\n\n\n# ============================================================================\n# SECTION 3: Inference Class\n# ============================================================================\n\nclass KidneyEvaluator:\n    \"\"\"Kidney Segmentation Inference Class\"\"\"\n    \n    def __init__(self, model_path, device='cuda'):\n        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n        self.model = self.load_model(model_path)\n        self.model.eval()\n        print(f\"Model loaded successfully on {self.device}\")\n        \n    def load_model(self, model_path):\n        \"\"\"Load trained model from checkpoint\"\"\"\n        checkpoint = torch.load(model_path, map_location=self.device)\n        model = EnhancedUNet(in_channels=1, out_channels=1)\n        \n        # Count parameters\n        total_params = sum(p.numel() for p in model.parameters())\n        print(f\"Total number of parameters: {total_params:,}\")\n        \n        # Load state dict\n        model.load_state_dict(checkpoint['model_state_dict'])\n        model = model.to(self.device)\n        \n        return model\n    \n    def preprocess_image(self, image):\n        \"\"\"Preprocess image for model input\"\"\"\n        # Convert to grayscale if needed\n        if len(image.shape) == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        \n        # Resize to model input size\n        image = cv2.resize(image, (256, 256))\n        \n        # Normalize and convert to tensor\n        image = image.astype(np.float32) / 255.0\n        image = torch.from_numpy(image).unsqueeze(0).unsqueeze(0)\n        \n        return image.to(self.device)\n    \n    def process_single_image(self, image_path, save_path=None, show_plot=True):\n        \"\"\"Process a single image and return segmentation results\"\"\"\n        \n        # Read image\n        if not os.path.exists(image_path):\n            raise FileNotFoundError(f\"Image not found: {image_path}\")\n            \n        original = cv2.imread(image_path)\n        if original is None:\n            raise ValueError(f\"Failed to read image: {image_path}\")\n            \n        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n        \n        # Store original dimensions\n        original_height, original_width = original.shape[:2]\n        \n        # Preprocess\n        start_time = datetime.datetime.now()\n        processed = self.preprocess_image(image)\n        \n        # Inference\n        with torch.no_grad():\n            output = self.model(processed)\n            probabilities = torch.sigmoid(output)\n            prediction = (probabilities > 0.5).float()\n        \n        end_time = datetime.datetime.now()\n        inference_time = (end_time - start_time).total_seconds()\n        \n        # Convert predictions back to numpy and resize to original dimensions\n        pred_mask = prediction.squeeze().cpu().numpy()\n        conf_map = probabilities.squeeze().cpu().numpy()\n        \n        # Resize predictions to match original image dimensions\n        pred_mask = cv2.resize(pred_mask, (original_width, original_height))\n        conf_map = cv2.resize(conf_map, (original_width, original_height))\n        \n        confidence_score = float(conf_map.mean())\n        \n        # Create visualization\n        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n        fig.suptitle(f'Inference Time: {inference_time:.4f}s | Confidence: {confidence_score:.3f}', \n                     fontsize=14, fontweight='bold')\n        \n        # Original\n        axes[0, 0].imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n        axes[0, 0].set_title('Original Image', fontsize=12)\n        axes[0, 0].axis('off')\n        \n        # Predicted Mask\n        axes[0, 1].imshow(pred_mask, cmap='gray')\n        axes[0, 1].set_title('Predicted Mask', fontsize=12)\n        axes[0, 1].axis('off')\n        \n        # Confidence Map\n        conf_map_display = axes[0, 2].imshow(conf_map, cmap='jet', vmin=0, vmax=1)\n        axes[0, 2].set_title('Confidence Map', fontsize=12)\n        axes[0, 2].axis('off')\n        plt.colorbar(conf_map_display, ax=axes[0, 2], fraction=0.046, pad=0.04)\n        \n        # Overlay\n        overlay = cv2.cvtColor(original, cv2.COLOR_BGR2RGB).copy()\n        mask_overlay = np.zeros_like(overlay)\n        mask_overlay[pred_mask > 0.5] = [255, 255, 0]  # Yellow overlay\n        overlay = cv2.addWeighted(overlay, 0.85, mask_overlay, 0.15, 0)\n        axes[1, 0].imshow(overlay)\n        axes[1, 0].set_title('Mask Overlay', fontsize=12)\n        axes[1, 0].axis('off')\n        \n        # Contour visualization\n        contour_img = cv2.cvtColor(original, cv2.COLOR_BGR2RGB).copy()\n        contours, _ = cv2.findContours((pred_mask > 0.5).astype(np.uint8), \n                                       cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cv2.drawContours(contour_img, contours, -1, (255, 255, 0), 2)\n        axes[1, 1].imshow(contour_img)\n        axes[1, 1].set_title('Contour Visualization', fontsize=12)\n        axes[1, 1].axis('off')\n        \n        # Statistics\n        axes[1, 2].axis('off')\n        stats_text = f\"\"\"\n        Image Statistics:\n        ─────────────────\n        Original Size: {original_width}x{original_height}\n        Model Input: 256x256\n        \n        Segmentation Metrics:\n        ─────────────────\n        Mean Confidence: {confidence_score:.3f}\n        Max Confidence: {conf_map.max():.3f}\n        Min Confidence: {conf_map.min():.3f}\n        \n        Mask Coverage: {(pred_mask > 0.5).sum() / pred_mask.size * 100:.2f}%\n        Number of Contours: {len(contours)}\n        \"\"\"\n        axes[1, 2].text(0.1, 0.5, stats_text, fontsize=10, \n                       verticalalignment='center', family='monospace')\n        \n        plt.tight_layout()\n        \n        # Save figure if save_path is provided\n        if save_path:\n            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            print(f\"Result saved to: {save_path}\")\n        \n        if show_plot:\n            plt.show()\n        else:\n            plt.close(fig)\n        \n        return {\n            'original': original,\n            'pred_mask': pred_mask,\n            'conf_map': conf_map,\n            'confidence_score': confidence_score,\n            'inference_time': inference_time,\n            'contours': contours,\n            'figure': fig\n        }\n    \n    def process_batch(self, image_paths, output_dir='./results'):\n        \"\"\"Process multiple images and save results\"\"\"\n        \n        os.makedirs(output_dir, exist_ok=True)\n        results = []\n        \n        print(f\"\\nProcessing {len(image_paths)} images...\")\n        \n        for img_path in tqdm(image_paths):\n            try:\n                img_name = os.path.basename(img_path)\n                save_path = os.path.join(output_dir, f'result_{img_name}')\n                \n                result = self.process_single_image(img_path, save_path=save_path, show_plot=False)\n                \n                results.append({\n                    'image': img_name,\n                    'confidence': result['confidence_score'],\n                    'inference_time': result['inference_time'],\n                    'num_contours': len(result['contours'])\n                })\n                \n            except Exception as e:\n                print(f\"\\nError processing {img_path}: {str(e)}\")\n                continue\n        \n        # Save summary\n        summary_path = os.path.join(output_dir, 'batch_summary.txt')\n        with open(summary_path, 'w') as f:\n            f.write('Kidney Segmentation - Batch Processing Summary\\n')\n            f.write('=' * 60 + '\\n\\n')\n            \n            for result in results:\n                f.write(f\"Image: {result['image']}\\n\")\n                f.write(f\"  Confidence: {result['confidence']:.3f}\\n\")\n                f.write(f\"  Inference Time: {result['inference_time']:.4f}s\\n\")\n                f.write(f\"  Number of Contours: {result['num_contours']}\\n\\n\")\n            \n            # Overall statistics\n            if results:\n                avg_confidence = np.mean([r['confidence'] for r in results])\n                avg_time = np.mean([r['inference_time'] for r in results])\n                \n                f.write('\\n' + '=' * 60 + '\\n')\n                f.write('Overall Statistics:\\n')\n                f.write(f\"  Total Images Processed: {len(results)}\\n\")\n                f.write(f\"  Average Confidence: {avg_confidence:.3f}\\n\")\n                f.write(f\"  Average Inference Time: {avg_time:.4f}s\\n\")\n        \n        print(f\"\\nBatch processing complete!\")\n        print(f\"Results saved to: {output_dir}\")\n        print(f\"Summary saved to: {summary_path}\")\n        \n        return results\n\n\n# ============================================================================\n# SECTION 4: Usage Examples\n# ============================================================================\n\ndef inference_single_image(model_path, image_path, save_path=None):\n    \"\"\"\n    Quick inference function for a single image\n    \n    Args:\n        model_path: Path to the trained model checkpoint\n        image_path: Path to the input image\n        save_path: Optional path to save the result\n    \"\"\"\n    evaluator = KidneyEvaluator(model_path)\n    result = evaluator.process_single_image(image_path, save_path=save_path)\n    return result\n\n\ndef inference_batch(model_path, image_dir, output_dir='./results'):\n    \"\"\"\n    Batch inference function for multiple images\n    \n    Args:\n        model_path: Path to the trained model checkpoint\n        image_dir: Directory containing input images\n        output_dir: Directory to save results\n    \"\"\"\n    # Get all image files\n    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n    image_paths = [\n        os.path.join(image_dir, f) \n        for f in os.listdir(image_dir) \n        if os.path.splitext(f)[1].lower() in valid_extensions\n    ]\n    \n    if not image_paths:\n        print(f\"No valid images found in {image_dir}\")\n        return\n    \n    evaluator = KidneyEvaluator(model_path)\n    results = evaluator.process_batch(image_paths, output_dir)\n    return results\n\n\n# ============================================================================\n# SECTION 5: Main Execution\n# ============================================================================\n\nif __name__ == \"__main__\":\n    # Configuration\n    MODEL_PATH = '/kaggle/input/kidneyseg/pytorch/default/20/best_model_checkpoint_1MW.pth'\n    \n    # Example 1: Single image inference\n    print(\"\\n\" + \"=\"*60)\n    print(\"EXAMPLE 1: Single Image Inference\")\n    print(\"=\"*60)\n    \n    IMAGE_PATH = '/kaggle/input/testimages/240_F_603611028_GGVVOAxWeuRbSFgkZVrcJIkMSLJfeTDG.jpg'\n    SAVE_PATH = '/kaggle/working/kidney_result.png'\n    \n    result = inference_single_image(MODEL_PATH, IMAGE_PATH, SAVE_PATH)\n    print(f\"\\nInference completed successfully!\")\n    print(f\"Confidence Score: {result['confidence_score']:.3f}\")\n    print(f\"Inference Time: {result['inference_time']:.4f}s\")\n    \n    # Example 2: Batch inference (uncomment to use)\n    # print(\"\\n\" + \"=\"*60)\n    # print(\"EXAMPLE 2: Batch Image Inference\")\n    # print(\"=\"*60)\n    # \n    # IMAGE_DIR = '/kaggle/input/testimages/'\n    # OUTPUT_DIR = '/kaggle/working/batch_results/'\n    # \n    # results = inference_batch(MODEL_PATH, IMAGE_DIR, OUTPUT_DIR)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Inference notebook execution completed!\")\n    print(\"=\"*60)","metadata":{"_uuid":"7dd1e1a4-a195-4c0e-be04-2027f8c0b5d5","_cell_guid":"95def486-140f-47e0-9805-66dacbddf44a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-31T07:02:35.044114Z","iopub.execute_input":"2025-12-31T07:02:35.044427Z","iopub.status.idle":"2025-12-31T07:02:49.281064Z","shell.execute_reply.started":"2025-12-31T07:02:35.044391Z","shell.execute_reply":"2025-12-31T07:02:49.280104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"a81d3a33-0fe2-4873-837b-3941fbd194a8","_cell_guid":"662636b0-3653-4092-bb52-212c9c41ea6b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}